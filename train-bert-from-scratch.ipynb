{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentencepiece\n!pip install -U tensorflow==1.15","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport nltk\nimport random\n\nimport tensorflow as tf\nimport sentencepiece as spm\n\nfrom glob import glob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect TPU\nif os.environ[\"TPU_NAME\"]:\n    USE_TPU = True\nelse:\n    USE_TPU = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/google-research/bert/archive/refs/heads/master.zip\n!unzip master.zip\n!mv bert-master/ bert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append(\"/kaggle/working/bert/\")\n\nfrom bert import modeling, optimization, tokenization\nfrom bert.run_pretraining import input_fn_builder, model_fn_builder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.en.gz -O dataset.txt.gz\n!gunzip dataset.txt.gz\n! ls -lh\n# !tailx -5f dataset.txt","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRIAL_RUN = True\nif TRIAL_RUN:\n    LINE_COUNT = 1000\n    FNAME = \"dataset_\" + str(LINE_COUNT) + \".txt\"\n    os.system(\"head -n \" + str(LINE_COUNT) + \" dataset.txt > \" + FNAME)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"dataset.txt\"\nif TRIAL_RUN:\n    DATA_PATH = FNAME\nprint(\"Data Path: {}\".format(DATA_PATH))\nMODEL_PREFIX = \"tokenizer\"\nVOC_SIZE = 800\nNUM_PLACEHOLDERS = 50\n\nSPM_COMMAND = ('--input={} --model_prefix={} '\n               '--vocab_size={} '\n               '--shuffle_input_sentence=true ' \n               '--bos_id=-1 --eos_id=-1').format(\n               DATA_PATH, MODEL_PREFIX, \n               VOC_SIZE - NUM_PLACEHOLDERS)\n\nspm.SentencePieceTrainer.Train(SPM_COMMAND)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_sentencepiece_vocab(filepath):\n    voc = []\n    with open(filepath, encoding='utf-8') as fi:\n        for line in fi:\n            voc.append(line.split(\"\\t\")[0])\n    # skip the first <unk> token\n    voc = voc[1:]\n    return voc\n\nsnt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\nprint(\"Learnt vocab size: {}\".format(len(snt_vocab)))\nprint(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_sentencepiece_token(token):\n    if token.startswith(\"‚ñÅ\"):\n        return token[1:]\n    else:\n        return \"##\" + token\n        \nbert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n\nctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\nbert_vocab = ctrl_symbols + bert_vocab\n\nbert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\nprint(len(bert_vocab))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOC_FNAME = \"en-vocab.txt\"\n\nwith open(VOC_FNAME, \"w\") as fo:\n    for token in bert_vocab:\n        fo.write(token+\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text = \"legal jurisdictions exercise their right to determine who is recognized as being a lawyer\"\ntokenizer = tokenization.FullTokenizer(VOC_FNAME)\ntokenizer.tokenize(sample_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir data_splits/\nif TRIAL_RUN:\n    !split -l 1000 -d dataset_1000.txt data_splits/data_\nelse:\n    !split -l 10000 -d dataset.txt data_splits/data_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQ_LENGTH = 128\nMASKED_LM_PROB = 0.15\nMAX_PREDICTIONS = 20\nDO_LOWER_CASE = True\n\nPRETRAINING_DIR = \"pretraining_data\"\n\nPROCESSES = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nXARGS_CMD = (\"ls data_splits/ | \"\n             \"xargs -n 1 -P {} -I{} \"\n             \"python bert/create_pretraining_data.py \"\n             \"--input_file=data_splits/{} \"\n             \"--output_file={}/{}.tfrecord \"\n             \"--vocab_file={} \"\n             \"--do_lower_case={} \"\n             \"--max_predictions_per_seq={} \"\n             \"--max_seq_length={} \"\n             \"--masked_lm_prob={} \"\n             \"--random_seed=42 \"\n             \"--dupe_factor=5\")\n\nXARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n                             VOC_FNAME, DO_LOWER_CASE, \n                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)\n                             \ntf.gfile.MkDir(PRETRAINING_DIR)\n!$XARGS_CMD","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUCKET_NAME = \"bert_resourses\"\nMODEL_DIR = \"bert_model\"\ntf.io.gfile.mkdir(MODEL_DIR)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_base_config = {\n  \"attention_probs_dropout_prob\": 0.1, \n  \"directionality\": \"bidi\", \n  \"hidden_act\": \"gelu\", \n  \"hidden_dropout_prob\": 0.1, \n  \"hidden_size\": 768, \n  \"initializer_range\": 0.02, \n  \"intermediate_size\": 3072, \n  \"max_position_embeddings\": 512, \n  \"num_attention_heads\": 12, \n  \"num_hidden_layers\": 12, \n  \"pooler_fc_size\": 768, \n  \"pooler_num_attention_heads\": 12, \n  \"pooler_num_fc_layers\": 3, \n  \"pooler_size_per_head\": 128, \n  \"pooler_type\": \"first_token_transform\", \n  \"type_vocab_size\": 2, \n  \"vocab_size\": VOC_SIZE\n}\n\nwith open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n    json.dump(bert_base_config, fo, indent=4)\n\n\nwith open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n    for token in bert_vocab:\n        fo.write(token+\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## upload content to GCS (authenticate here and upload from here or download data and model directory and upload to GCS to train on TPU)\nif BUCKET_NAME:\n  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUCKET_NAME = \"bert_resourses\"\nMODEL_DIR = \"bert_model\"\nPRETRAINING_DIR = \"pretraining_data\"\nVOC_FNAME = \"vocab.txt\"\n\n# Input data pipeline config\nTRAIN_BATCH_SIZE = 128\nMAX_PREDICTIONS = 20\nMAX_SEQ_LENGTH = 128\nMASKED_LM_PROB = 0.15\n\n# Training procedure config\nEVAL_BATCH_SIZE = 64\nLEARNING_RATE = 2e-5\nTRAIN_STEPS = 100\nSAVE_CHECKPOINTS_STEPS = 25\nNUM_TPU_CORES = 8\n\nif BUCKET_NAME:\n    BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\nelse:\n    BUCKET_PATH = \".\"\n\nBERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\nDATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n\nVOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\nCONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n\nINIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n\nbert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\ninput_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fn = model_fn_builder(\n      bert_config=bert_config,\n      init_checkpoint=INIT_CHECKPOINT,\n      learning_rate=LEARNING_RATE,\n      num_train_steps=TRAIN_STEPS,\n      num_warmup_steps=10,\n      use_tpu=USE_TPU,\n      use_one_hot_embeddings=True)\n\ntpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=BERT_GCS_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=USE_TPU,\n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)\n  \ntrain_input_fn = input_fn_builder(\n        input_files=input_files,\n        max_seq_length=MAX_SEQ_LENGTH,\n        max_predictions_per_seq=MAX_PREDICTIONS,\n        is_training=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)","metadata":{},"execution_count":null,"outputs":[]}]}