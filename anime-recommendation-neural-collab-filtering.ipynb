{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! Y | apt-get install libblas-dev","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_anime = pd.read_csv(\"../input/anime-recommendations-database/anime.csv\")\ndf_rating = pd.read_csv(\"../input/anime-recommendations-database/rating.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_anime.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_rating.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_anime_feature_map(df_anime):\n    ## cleaning names\n    # df_anime['name'] = df_anime['name'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', re.sub(r'&#(\\d)+;', '', x)))\n    # df_anime = df_anime[df_anime['name'] != '']\n    \n    # ## Imputing episodes based on type of anime(mean value)\n    tmp = df_anime[df_anime.episodes != 'Unknown'][['type', 'episodes']]\n    tmp['episodes'] = tmp['episodes'].astype(int)\n    tmp = tmp.groupby('type').mean().to_dict()['episodes']\n    df_anime['episodes'] = df_anime.apply(lambda x: tmp.get(x['type'], 1) if (x['episodes'] == 'Unknown') else x['episodes'], axis=1)\n    df_anime['episodes'] = df_anime['episodes'].astype(int)\n    \n    ## Imputing rating with the mean rating\n    df_anime['rating'] = df_anime['rating'].fillna(df_anime['rating'].mean())\n    \n    #Imputing genre with extra '' class\n    df_anime['genre'] = df_anime['genre'].apply(lambda x: [g.strip() for g in (x.split(',') if (type(x) == str) else [''])])\n    mat = df_anime.to_numpy()\n    genres = mat[:,2]\n    \n    mlb = MultiLabelBinarizer()\n    mlb.fit(genres)\n    \n    ## Imputing type column with extra '' class\n    df_anime['type'] = df_anime['type'].fillna('')\n    \n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(np.array(list(set(df_anime['type']))).reshape(-1, 1))\n    \n    df_anime['genre'] = df_anime['genre'].apply(lambda x: mlb.transform([x])[0])\n    df_anime['type'] = df_anime['type'].apply(lambda x: ohe.transform([[x]])[0])\n    \n    ## normalize ratings and members\n    df_anime['rating'] = (df_anime['rating'] - df_anime['rating'].min())/(df_anime['rating'].max()-df_anime['rating'].min())\n    df_anime['members'] = (df_anime['members'] - df_anime['members'].min())/(df_anime['members'].max()-df_anime['members'].min())\n    \n    ## generating feature_map\n    anime_feature_map = {}\n    for idx, row in tqdm(df_anime.iterrows()):\n        anime_feature_map[row[\"anime_id\"]] = list(row[\"genre\"]) + list(row[\"type\"]) + [row[\"rating\"], row[\"members\"]]\n        \n    return anime_feature_map, mlb, ohe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anime_feature_map, mlb, ohe = get_anime_feature_map(df_anime)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_rating.head()\ndf_rating['anime_features'] = df_rating['anime_id'].apply(lambda x: anime_feature_map.get(x))\ndf_rating = df_rating[~df_rating.anime_features.isna()]\ndf_rating = df_rating[df_rating['rating'] != -1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_rating.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_count = df_rating.groupby('user_id').count()['rating']\ndf_rating = df_rating[df_rating['user_id'].apply(lambda x: 10 <= user_count[x] <= 30)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_idx_map = {u: e for e, u in enumerate(df_rating.user_id.unique())}\nanime_idx_map = {i: e for e, i in enumerate(df_rating.anime_id.unique())}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_rating[\"user_idx\"] = df_rating[\"user_id\"].apply(lambda x: user_idx_map[x])\ndf_rating[\"anime_idx\"] = df_rating[\"anime_id\"].apply(lambda x: anime_idx_map[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_rating[\"user_idx\"].max())\nprint(df_rating[\"anime_idx\"].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_rating.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_rating_train, df_rating_test = train_test_split(df_rating, test_size=0.1, stratify=df_rating.user_id, random_state=93)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = [df_rating_train['user_idx'].tolist(), df_rating_train['anime_idx'].tolist(), df_rating_train['anime_features'].tolist()]\ny_train = df_rating_train['rating']\n\nX_test = [df_rating_test['user_idx'].tolist(), df_rating_test['anime_idx'].tolist(), df_rating_test['anime_features'].tolist()]\ny_test = df_rating_test['rating']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.disable_v2_behavior()\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Input, Embedding, Dot, Concatenate, Add, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(n_users, user_embed_size_dot, user_embed_size_concat, n_items, item_embed_size, item_feature_len, regularization=1e-4):\n     \n    item_features = Input(shape=(item_feature_len, ), name=\"item_features\")\n    user_inp = Input(shape=(1, ), dtype='int8', name=\"user_embed\")\n    user_embed = Embedding(n_users, \n                           user_embed_size_dot, \n                           name='user_embed_mat',\n                           embeddings_initializer=\"glorot_uniform\", \n                           embeddings_regularizer=keras.regularizers.l2(regularization))(user_inp)\n    user_embed_bias = Embedding(n_users, \n                                1, \n                                name='user_embed_bias_mat',\n                                embeddings_initializer=\"glorot_uniform\")(user_inp)\n    user_embed_c = Embedding(n_users, \n                             user_embed_size_concat, \n                             name='user_embed_c_mat',\n                             embeddings_initializer=\"glorot_uniform\", \n                             embeddings_regularizer=keras.regularizers.l2(regularization))(user_inp)\n    \n    item_inp = Input(shape=(1, ), dtype='int8', name=\"item_embed\")\n    item_embed = Embedding(n_items, \n                           item_embed_size, \n                           name='item_embed_mat',\n                           embeddings_initializer=\"glorot_uniform\", \n                           embeddings_regularizer=keras.regularizers.l2(regularization))(item_inp)\n    item_embed_bias = Embedding(n_items, \n                                1, \n                                name='item_embed_bias_mat',\n                                embeddings_initializer=\"glorot_uniform\")(item_inp)\n    \n    user_item_dot = Dot(axes=2, name='user_item_dot')([user_embed, item_embed])\n    \n    user_item_dot = Add()([user_item_dot, user_embed_bias, item_embed_bias])\n    user_item_dot = Flatten()(user_item_dot)\n    user_embed_c = Flatten()(user_embed_c)\n    \n    user_item_concat = Concatenate(axis=1)([user_embed_c, item_features])\n    \n    hidden1 = Dense(8, activation=\"relu\")(user_item_concat)\n    hidden1 = BatchNormalization()(hidden1)\n    hidden1 = Dropout(0.2)(hidden1)\n    \n    dot_hidden1_concat = Concatenate(axis=1)([hidden1, user_item_dot])\n    \n    output = Dense(1, activation=\"relu\")(dot_hidden1_concat)\n    \n    model = Model([user_inp, item_inp, item_features], output)\n    \n    return model\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_USERS = df_rating.user_idx.max()\nN_ITEMS = df_rating.anime_idx.max()\nUSER_EMBEDDING_SIZE_DOT = 20\nUSER_EMBEDDING_SIZE_CONCAT = 20\nITEM_EMBEDDING_SIZE = 20\nITEM_FEATURE_LEN = 53\n\nmodel = create_model(N_USERS, USER_EMBEDDING_SIZE_DOT, USER_EMBEDDING_SIZE_CONCAT, N_ITEMS, ITEM_EMBEDDING_SIZE, ITEM_FEATURE_LEN)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks defined\n\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.001\n    drop = 0.5\n    epochs_drop = 5\n    lrate = initial_lrate * (drop**((1 + epoch)/epochs_drop))\n    return lrate\n\nlrate_scheduler = LearningRateScheduler(step_decay)\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nmodel_chkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n\n# model fitting\nmodel.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.1, callbacks=[early_stop, model_chkpoint, lrate_scheduler])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_rating_test['prediction'] = [t[0] for t in model.predict(X_test)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_rating_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test MAE: {}\".format(sum(abs(df_rating_test[\"rating\"] - df_rating_test[\"prediction\"]))/len(df_rating_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}